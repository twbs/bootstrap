# www.robotstxt.org

{{- $isProduction := eq (getenv "HUGO_ENV") "production" -}}
{{- $isNetlify := eq (getenv "NETLIFY") "true" -}}
{{- $allowCrawling := and (not $isNetlify) $isProduction -}}

{{ if $allowCrawling }}
# Allow crawling of all content
{{- end }}
User-agent: *
Sitemap: {{ "/sitemap.xml" | absURL }}
Disallow:{{ if not $allowCrawling }} /{{ end }}

{{ if $allowCrawling -}}
{{- range $release := index $.Site.Data "docs-versions" -}}
{{- range $version := $release.versions -}}
{{- $verNum := index (split $version.v ".") 0 -}}
{{- if lt $verNum 3 -}}
Disallow: /{{ $version.v }}/
{{ else if not (eq $version.v $.Site.Params.docs_version) -}}
Disallow: /docs/{{ $version.v }}/
{{ end }}
{{- end -}}
{{- end -}}
{{- end -}}
